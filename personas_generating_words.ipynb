{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_plots = pd.read_csv(\n",
    "    'MovieSummaries/plot_summaries.txt',\n",
    "    delimiter='\\t',\n",
    "    names=['id', 'plot']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_verbs_args = ['nsubj', 'agent']\n",
    "patient_verbs_args = ['dobj', 'nsubjpass', 'iobj', 'prep_']\n",
    "attributes_governors_args = ['nsubj', 'appos']\n",
    "attributes_dependents_args = ['nsubj', 'appos', 'amod', 'nn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mention(mention, root):\n",
    "    if (mention.attrib and mention.attrib['representative'] == 'true'):\n",
    "        print('This is the representative mention')\n",
    "    sentence_id = mention.find('sentence').text\n",
    "    start = int(mention.find('start').text)\n",
    "    end = int(mention.find('end').text)\n",
    "    head = int(mention.find('head').text)\n",
    "    for element in root.findall('./document/sentences/sentence/[@id=\"'+sentence_id+'\"]/tokens/token'):\n",
    "        if int(element.attrib['id']) in range(start, end):\n",
    "            if int(element.attrib['id']) == head:\n",
    "                head_word = element.find('word').text\n",
    "            print(element.find('word').text, end=\" \")\n",
    "    print(\" \")\n",
    "    print(\"Head word is: \", head_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dependencies(root):\n",
    "    dependencies = []\n",
    "    for sentence in root.findall(\"./document/sentences/sentence\"):\n",
    "        for dependency in sentence.findall(\"./collapsed-ccprocessed-dependencies/dep\"):\n",
    "            if (dependency.attrib['type'] in agent_verbs_args + patient_verbs_args + attributes_governors_args + attributes_dependents_args):\n",
    "                dependencies.append(((int(sentence.attrib['id'])), int(dependency.find('dependent').attrib['idx']), int(dependency.find('governor').attrib['idx']), dependency.attrib['type']))\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_person_rw_couples(dependencies, people, root):\n",
    "    rw_bag = []\n",
    "    for (s, dep, gov, typ) in dependencies: \n",
    "        # Replace dep and gov by the head of the representative mention, if applicable\n",
    "        filtered_corefs = list(filter(lambda x: any([(int(mention.find('./head').text) in [dep, gov]) and (\n",
    "                        s == int(mention.find('./sentence').text)) for \n",
    "                        mention in x.findall('./mention')]) , \n",
    "                        root.findall(\"./document/coreference/coreference\")))\n",
    "        dep_sentence = s\n",
    "        gov_sentence = s\n",
    "        if len(filtered_corefs) > 0:\n",
    "            dep_heads = []\n",
    "            gov_heads = []\n",
    "            for coref in filtered_corefs:\n",
    "                dep_mentions = list(filter(lambda mention: (int(mention.find('./head').text) == dep) and (\n",
    "                            s == int(mention.find('./sentence').text)), \n",
    "                            coref.findall(\"./mention\")))\n",
    "                gov_mentions = list(filter(lambda mention: (int(mention.find('./head').text) == gov) and (\n",
    "                            s == int(mention.find('./sentence').text)), \n",
    "                            coref.findall(\"./mention\")))\n",
    "                head = int(coref.find(\"./mention/[@representative='true']/head\").text)\n",
    "                head_sentence = int(coref.find(\"./mention/[@representative='true']/sentence\").text)\n",
    "                if len(dep_mentions) > 0:\n",
    "                    dep_heads.append((head, head_sentence))\n",
    "                if len(gov_mentions) > 0:\n",
    "                    gov_heads.append((head, head_sentence))\n",
    "                \n",
    "            # if len(set(dep_heads)) > 1:\n",
    "                # print('More than 1 representative for dep')\n",
    "            # elif len(set(gov_heads)) > 1:\n",
    "                # print('More than 1 representative for gov')\n",
    "            # else:\n",
    "            if len(set(dep_heads)) == 1:\n",
    "                dep, dep_sentence = dep_heads[0]\n",
    "            if len(set(gov_heads)) == 1:\n",
    "                gov, gov_sentence = gov_heads[0]\n",
    "\n",
    "        r = ''\n",
    "\n",
    "        # Output (person, [word, type_identifier])\n",
    "        if ((typ in patient_verbs_args) or ('prep_' in typ)):\n",
    "            r = 'patient'\n",
    "        elif((typ in agent_verbs_args) and (root.find(\"./document/sentences/sentence/[@id='\"+str(s)+\"']/tokens/token/[@id='\"+str(gov)+\"']/POS\"\n",
    "                    ) and \"VB\" in root.find(\"./document/sentences/sentence/[@id='\"+str(s)+\"']/tokens/token/[@id='\"+str(gov)+\"']/POS\").text)):\n",
    "            r = 'agent'\n",
    "        elif(typ in attributes_dependents_args):\n",
    "            r = 'attribute'\n",
    "\n",
    "        # Append bag of (r,w)\n",
    "        if r != '':\n",
    "            if (r in ['agent', 'patient']) or (typ in attributes_governors_args) :\n",
    "                rw_bag.append((root.find(\"./document/sentences/sentence/[@id='\"+str(dep_sentence)+\"']/tokens/token/[@id='\"+str(dep)+\"']/lemma\").text,\n",
    "                        (r, root.find(\"./document/sentences/sentence/[@id='\"+str(gov_sentence)+\"']/tokens/token/[@id='\"+str(gov)+\"']/lemma\").text)))\n",
    "            if (r == 'attribute'):\n",
    "                rw_bag.append((root.find(\"./document/sentences/sentence/[@id='\"+str(gov_sentence)+\"']/tokens/token/[@id='\"+str(gov)+\"']/lemma\").text,\n",
    "                        (r, root.find(\"./document/sentences/sentence/[@id='\"+str(dep_sentence)+\"']/tokens/token/[@id='\"+str(dep)+\"']/lemma\").text)))\n",
    "\n",
    "    # Return (r,w) couples related to people\n",
    "    return list(filter(lambda x: x[0] in people, rw_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42303it [1:11:03,  9.92it/s]\n"
     ]
    }
   ],
   "source": [
    "words_per_movie = []\n",
    "for (i,j) in tqdm(enumerate(movie_plots['id'])):\n",
    "    words_per_character = []\n",
    "    with gzip.open('corenlp_plot_summaries/' + str(j) + '.xml.gz', 'rt', encoding='utf-8') as f:\n",
    "        root = ET.fromstring(f.read())\n",
    "        characters = list(set(map(lambda x: x.find('word').text, list(filter(\n",
    "            lambda x: x.find('NER').text == 'PERSON', root.findall(\"./document/sentences/sentence/tokens/token\"))))))\n",
    "        dependencies = generate_dependencies(root)\n",
    "        rw_list = generate_person_rw_couples(dependencies, characters, root)\n",
    "        rw_per_character = []\n",
    "        for person in characters:\n",
    "            rw_per_character.append((person, list(set(map(lambda x: x[1], list(filter(lambda x: x[0] == person, rw_list)))))))\n",
    "        rw_per_character = list(filter(lambda x: len(x[1]) > 0, rw_per_character))\n",
    "        words_per_character = words_per_character + rw_per_character\n",
    "    words_per_movie.append((j, words_per_character))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The total number of characters is: {}.'.format(len(words_per_character)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_per_character[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.Element(\"root\")\n",
    "doc = ET.SubElement(root, \"document\")\n",
    "\n",
    "for j in range(len(words_per_movie)):\n",
    "    movie_id = words_per_movie[j][0]\n",
    "    words_per_character = words_per_movie[j][1]\n",
    "    mov = ET.SubElement(doc, \"movie\", id=str(movie_id))\n",
    "    for i in range(len(words_per_character)):\n",
    "        character = words_per_character[i][0]\n",
    "        character_words = words_per_character[i][1]\n",
    "        ch = ET.SubElement(mov, \"character\", name=character)\n",
    "        for (r, w) in character_words:\n",
    "            ET.SubElement(ch, r).text = w\n",
    "\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"generated_character_words_per_movie.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree\n",
    "import lxml.builder    \n",
    "\n",
    "E = lxml.builder.ElementMaker()\n",
    "ROOT = E.root\n",
    "DOC = E.document\n",
    "CHARACTER = E.character\n",
    "WORD = E.word\n",
    "\n",
    "doc = ROOT( DOC() ) \n",
    "\n",
    "for j in range(len(words_per_movie)):\n",
    "    movie_id = words_per_movie[j][0]\n",
    "    words_per_character = words_per_movie[j][1]\n",
    "    for i in range(len(words_per_character)):\n",
    "        character = words_per_character[i][0]\n",
    "        character_words = words_per_character[i][1]\n",
    "        ch = E.character(\n",
    "            E.name(character),\n",
    "            E.movie_id(str(movie_id)),\n",
    "            *(E.word(w, name=r) for (r,w) in character_words)\n",
    "        )\n",
    "        doc.append(ch)\n",
    "\n",
    "pretty_output = open(\"generated_character_words_prettyprint_per_movie.xml\", \"w\")\n",
    "pretty_output.write(lxml.etree.tostring(doc, pretty_print=True).decode())\n",
    "pretty_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d31f9181dd6cb9cab03d486bdaa43f98f0f9995ccb58e866bbb38b2bb175b2b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
